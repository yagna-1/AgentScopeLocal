# ğŸ•µï¸ AgentScope Local

**Privacy-First LLM Observability & Debugging for Local Development**

Stop debugging with `print()` statements. AgentScope Local is your AI agent's Flight Recorderâ€”capturing every LLM call, RAG operation, and performance metric, all running locally on your machine with **zero cloud dependencies**.

```python
import agentscope as ag

ag.init()  # One line to start!

# Your LLM calls are now automatically traced with beautiful terminal output
response = ag.llm.chat("ollama/qwen2.5:0.5b", "What is AI?")
```

<p align="center">
  <img src="docs/images/ui_screenshot.png" alt="AgentScope Terminal UI" width="800"/>
</p>

---

## âœ¨ **Why AgentScope?**

**ğŸ¯ 2-Line Integration** - Add observability to any AI app in seconds  
**ğŸ¨ Beautiful Terminal UI** - Rich, colorful metrics without opening a browser  
**ğŸ”’ 100% Local** - Your data never leaves your machine  
**ğŸŒ Framework Agnostic** - Works with LangChain, LlamaIndex, CrewAI, AutoGPT, and 25+ more  
**âš¡ Zero Overhead** - Non-invasive Python decorators  
**ğŸ” RAG Debugging** - See exactly which documents were retrieved and why  
**ğŸ“Š Performance Metrics** - TTFT, TPS, latency, resource usage  
**ğŸŒŠ Streaming Support** - Real-time token visualization

---

## ğŸš€ **Quick Start**

### Installation

```bash
pip install -r requirements.txt
```

### Basic Usage

```python
import agentscope as ag

# 1. Initialize (terminal mode by default)
ag.init()

# 2. Your code runs normally - tracing happens automatically!
@ag.trace
def my_rag_pipeline(query: str):
    docs = retrieve_documents(query)
    return generate_answer(docs, query)

# 3. See beautiful output in your terminal
result = my_rag_pipeline("What is machine learning?")

# 4. Optional: Open web UI for deep inspection
ag.web.open()  # Auto-opens browser to http://localhost:8000
```

**That's it!** Your terminal now shows:

- ğŸ“¡ LLM calls with model, tokens, TTFT, speed
- ğŸ” RAG retrievals with similarity scores
- âš¡ Performance metrics (CPU, memory, GPU)
- ğŸŒŠ Streaming visualization
- ğŸ“Š Session summaries

---

## ğŸ¨ **Beautiful Terminal UI**

AgentScope features a **stunning terminal interface** with vibrant colors, progress bars, and comprehensive metrics:

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ•µï¸  AgentScope Local                                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ¯ Terminal Mode          â”‚  ğŸ“Š Live Trace Visualization    â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  âœ¨ Performance â”‚ ğŸ” RAG Debug â”‚ âš¡ Streaming â”‚ ğŸ’» Resources  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¤– ollama_call
  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  ğŸ“¡ LLM CALL                                            â•‘
  â•‘  ğŸ”· Provider      OLLAMA                                â•‘
  â•‘  ğŸ¤– Model         qwen2.5:0.5b                          â•‘
  â•‘  ğŸ’¬ Tokens        35 â†’ 50 (85 total)                    â•‘
  â•‘  âš¡ TTFT          729ms                                 â•‘
  â•‘  ğŸš€ Speed         116.6 tok/s                           â•‘
  â•‘  ğŸŒ¡ï¸  Temperature   â–®â–®â–®â–¯â–¯â–¯â–¯â–¯â–¯â–¯ 0.7                       â•‘
  â•‘  âš™ï¸  CPU           â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 15%                  â•‘
  â•‘  ğŸ§  RAM           82 MB                                 â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  âœ“ COMPLETE
```

---

## ğŸŒ **Framework Compatibility**

AgentScope works with **100% of Python AI frameworks**. Verified compatible with:

### LLM Orchestration

âœ… **LangChain** | **LangGraph** | **LlamaIndex** | **Haystack** | **DSPy**

### RAG Frameworks

âœ… **GraphRAG** | **LightRAG** | **RAGFlow** | **R2R** | **Morphik** | **Pathway**

### Agent Frameworks

âœ… **CrewAI** | **AutoGPT** | **AgentGPT**

### Vector Databases

âœ… **Pinecone** | **Weaviate** | **Milvus** | **Qdrant** | **Chroma** | **pgvector**

### Direct LLM SDKs

âœ… **OpenAI** | **Anthropic** | **Ollama** | **HuggingFace** | **Cohere**

**Why?** Python decorators work with ANY code. No framework dependencies required.

[ğŸ“„ Full Compatibility Report](docs/framework_compatibility.md)

---

## ğŸ“š **Integration Examples**

### LangChain Integration

```python
import agentscope as ag
from langchain_community.llms import Ollama
from langchain.chains import LLMChain

ag.init()

@ag.trace(name="langchain_pipeline")
def run_chain(question: str):
    llm = Ollama(model="llama3")
    chain = LLMChain(llm=llm, prompt=prompt_template)
    return chain.run(question=question)

# AgentScope automatically traces everything!
result = run_chain("What is AI?")
```

### LlamaIndex RAG

```python
import agentscope as ag
from llama_index.core import VectorStoreIndex

ag.init()

@ag.trace(name="llamaindex_rag")
def query_docs(question: str):
    with ag.span("indexing"):
        index = VectorStoreIndex.from_documents(documents)

    with ag.span("retrieval"):
        query_engine = index.as_query_engine()
        response = query_engine.query(question)

    return response
```

### CrewAI Multi-Agent

```python
import agentscope as ag
from crewai import Agent, Task, Crew

ag.init()

@ag.trace(name="crew_workflow")
def run_crew():
    agent = Agent(role="Researcher", llm=llm)
    task = Task(description="Research AI trends", agent=agent)
    crew = Crew(agents=[agent], tasks=[task])
    return crew.kickoff()
```

### Custom RAG System

```python
import agentscope as ag

ag.init()

@ag.trace(name="custom_rag")
def my_rag(query: str):
    # Embed query
    with ag.span("embedding"):
        query_vec = embed(query)
        ag.log_rag_embedding(query, query_vec, type="query")

    # Retrieve documents
    with ag.span("retrieval"):
        docs = vector_db.search(query_vec, top_k=5)
        for doc in docs:
            ag.log_rag_embedding(
                doc.text, doc.vector,
                type="document",
                metadata={"score": doc.score}
            )

    # Generate answer
    return ag.llm.chat("ollama/llama3", f"Context: {docs}\n\nQ: {query}")
```

---

## ğŸ¯ **Core Features**

### ğŸ“Š Performance Metrics

- **Time to First Token (TTFT)** - How fast your LLM responds
- **Tokens Per Second (TPS)** - Generation throughput
- **Total Generation Time** - End-to-end latency
- **Context Window Usage** - Visual progress bars

### ğŸ’» Resource Monitoring

- **CPU Usage** - Per-request CPU consumption
- **Memory Tracking** - RAM usage patterns
- **GPU Utilization** - CUDA memory and usage (if available)

### ğŸŒŠ Streaming Support

- **Real-time Token Display** - See tokens as they stream
- **Chunk Timing** - Inter-chunk latency analysis
- **Per-Token Latency** - Individual token generation speed

### ğŸ” RAG Debugging

- **Vector Inspection** - See embeddings and similarity scores
- **Document Retrieval** - Which docs were retrieved?
- **Relevance Scores** - Why were they chosen?

### â° Time Travel

- **Replay LLM Calls** - "Fork" a past call
- **Modify Parameters** - Change prompt, temperature, model
- **A/B Testing** - Compare different configurations

---

## ğŸ› ï¸ **Advanced Usage**

### Manual Spans for Fine Control

```python
with ag.span("custom_operation") as span:
    span.set_attribute("user_id", "123")
    result = process_data()
    span.set_metric("items_processed", len(result))
```

### Direct LLM Wrapper (Auto-Tracing)

```python
# Built-in wrapper handles tracing automatically
response = ag.llm.chat(
    model="ollama/qwen2.5:0.5b",
    prompt="Explain quantum computing",
    temperature=0.7,
    max_tokens=500,
    stream=True  # Live streaming in terminal!
)

print(response.text)
print(f"TTFT: {response.metrics['ttft_ms']}ms")
```

### Web UI for Deep Inspection

```python
# Terminal mode by default
ag.init()

# Make some LLM calls...
response = ag.llm.chat("ollama/llama3", "Hello")

# Open web UI when you need deep analysis
ag.web.open()  # Auto-opens browser, keeps terminal running
```

---

## ğŸ“– **Example Code**

Check out the `/examples` directory:

- **`package_mode_example.py`** - Minimal integration demo
- **`simple_rag_chat.py`** - Full RAG chatbot with retrieval
- **`test_web_open.py`** - Web UI integration test
- **`ui_demo.py`** - Terminal UI showcase

Run any example:

```bash
python examples/package_mode_example.py
```

---

## ğŸ—ï¸ **Architecture**

AgentScope Local uses **OpenTelemetry** for instrumentation and stores traces in **SQLite** for local analysis.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your AI App    â”‚
â”‚  (LangChain,    â”‚
â”‚   LlamaIndex,   â”‚
â”‚   Custom RAG)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ @ag.trace decorator
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AgentScope     â”‚
â”‚  Instrumentationâ”‚
â”‚  (OpenTelemetry)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Terminalâ”‚ â”‚SQLiteâ”‚
â”‚  UI    â”‚ â”‚  DB  â”‚
â”‚(Rich)  â”‚ â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”˜
              â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Web UI  â”‚
         â”‚ (FastAPI)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Components:**

- **Session Manager** - In-process telemetry (no standalone server!)
- **Terminal UI** - Rich-formatted live output
- **SQLite Exporter** - Local trace storage
- **Web UI** - Optional browser-based inspector
- **LLM Wrappers** - Auto-instrumented clients for Ollama, OpenAI, Anthropic

---

## ğŸ“ **Tips & Best Practices**

### For Best Terminal Experience

```python
# Terminal mode shows live traces
ag.init()  # or ag.init(mode="terminal")
```

### For Web-First Workflow

```python
# Web mode launches browser immediately
ag.init(mode="web", port=8000)
```

### For Headless/CI Environments

```python
# Headless mode: collect data, no UI
ag.init(mode="headless")
```

### RAG Debugging Tips

```python
# Always log embeddings for RAG debugging
ag.log_rag_embedding(
    text=query,
    vector=query_embedding,
    type="query"  # or "document"
)
```

### Performance Optimization

- Use `@ag.trace` sparingly on critical paths
- Set `mode="headless"` in production for minimal overhead
- Configure context window limits to avoid token waste

---

## ğŸ”§ **Configuration**

### Environment Variables

```bash
# Database location (default: agentscope_traces.db)
export AGENTSCOPE_DB_PATH=/path/to/traces.db

# Web UI port (default: 8000)
export AGENTSCOPE_PORT=9000
```

### Python Configuration

```python
ag.init(
    mode="terminal",              # "terminal", "web", or "headless"
    db_path="my_traces.db",       # SQLite database path
    service_name="my_app",        # Service identifier
    port=8000                     # Web UI port
)
```

---

## ğŸ› **Troubleshooting**

### Ollama Not Found

```bash
# Install Ollama from https://ollama.ai
# Then pull a model:
ollama pull qwen2.5:0.5b
```

### Web UI Not Opening

```python
# Check if port is already in use
ag.init(mode="web", port=8001)  # Try different port
```

### Import Errors

```bash
# Ensure all dependencies installed
pip install -r requirements.txt
```

---

## ğŸ“„ **License**

MIT License - See [LICENSE](LICENSE) file

---

## ğŸ¤ **Contributing**

We welcome contributions! Areas we'd love help with:

- Additional LLM provider integrations
- UI/UX improvements
- Documentation & examples
- Bug reports & feature requests

---

## â­ **Star Us on GitHub!**

If AgentScope Local helps you debug your AI agents, give us a star! â­

---

**Built with â¤ï¸ for the local LLM community**

_Stop guessing. Start seeing. Debug your AI agents like a pro._ ğŸ•µï¸
